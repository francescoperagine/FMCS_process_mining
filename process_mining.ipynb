{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "import os\n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(file_path):\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#     df = pd.DataFrame([line.split() for line in lines], columns=[\"Date\", \"Time\", \"org:resource\", \"lifecycle:transition\", \"concept:name\"])\n",
    "\n",
    "#     df[\"time:timestamp\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], format='ISO8601')\n",
    "#     df['case:concept:name'] = df['time:timestamp'].dt.date.astype(str)\n",
    "\n",
    "#     df = df.drop(columns=[\"Date\", \"Time\"])\n",
    "\n",
    "#     return df[[\"case:concept:name\", \"time:timestamp\", \"concept:name\", \"org:resource\", \"lifecycle:transition\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(file_path):\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#     df = pd.DataFrame([line.split() for line in lines], columns=[\"Date\", \"Time\", \"org:resource\", \"lifecycle:transition\", \"concept:name\"])\n",
    "    \n",
    "#     # Convert to datetime\n",
    "#     df[\"time:timestamp\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], format='ISO8601')\n",
    "    \n",
    "#     # Sort by timestamp\n",
    "#     df = df.sort_values('time:timestamp')\n",
    "    \n",
    "#     # Calculate time differences between consecutive events\n",
    "#     time_diff = df['time:timestamp'].diff()\n",
    "    \n",
    "#     # Create new session when:\n",
    "#     # 1. Gap between events exceeds threshold (e.g., 4 hours)\n",
    "#     # 2. A new Sleep ON event starts\n",
    "#     new_session = (\n",
    "#         (time_diff > pd.Timedelta(hours=4)) | \n",
    "#         ((df['concept:name'] == 'Sleep') & (df['lifecycle:transition'] == 'ON'))\n",
    "#     )\n",
    "    \n",
    "#     # Create case IDs based on sessions\n",
    "#     df['case:concept:name'] = new_session.cumsum().astype(str)\n",
    "    \n",
    "#     # Keep required columns in correct order\n",
    "#     df = df[[\n",
    "#         'case:concept:name',\n",
    "#         'time:timestamp',\n",
    "#         'concept:name',\n",
    "#         'org:resource',\n",
    "#         'lifecycle:transition'\n",
    "#     ]]\n",
    "\n",
    "#     print(df['concept:name'].unique())\n",
    "#     print(df['org:resource'].unique())\n",
    "    \n",
    "#     # Verify dtypes\n",
    "#     print(\"DataFrame dtypes after preparation:\", df.dtypes)\n",
    "#     print(\"\\nFirst few rows of prepared data:\")\n",
    "#     print(df.head())\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    df = pd.DataFrame([line.split() for line in lines], \n",
    "                     columns=[\"Date\", \"Time\", \"org:resource\", \n",
    "                             \"lifecycle:transition\", \"concept:name\"])\n",
    "    \n",
    "    # Convert to datetime and sort chronologically\n",
    "    df[\"time:timestamp\"] = pd.to_datetime(df[\"Date\"] + \" \" + df[\"Time\"], \n",
    "                                        format='ISO8601')\n",
    "    df = df.sort_values('time:timestamp')\n",
    "    \n",
    "    # Calculate time differences between consecutive events\n",
    "    time_diff = df['time:timestamp'].diff()\n",
    "    \n",
    "    # Start new session when:\n",
    "    # 1. There's a gap of more than 4 hours between activities\n",
    "    # 2. A Sleep activity starts\n",
    "    new_session = (\n",
    "        (time_diff > pd.Timedelta(hours=4)) | \n",
    "        ((df['concept:name'] == 'Sleep') & (df['lifecycle:transition'] == 'ON'))\n",
    "    )\n",
    "    \n",
    "    # Create session IDs\n",
    "    df['case:concept:name'] = new_session.cumsum().astype(str)\n",
    "    \n",
    "    # Drop unused columns and reorder\n",
    "    df = df.drop(columns=[\"Date\", \"Time\"])\n",
    "    \n",
    "    return df[[\"case:concept:name\", \"time:timestamp\", \"concept:name\", \n",
    "               \"org:resource\", \"lifecycle:transition\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_process_mining(log):\n",
    "    # Apply different miners\n",
    "\n",
    "    alpha_net, alpha_initial_marking, alpha_final_marking = alpha_miner.apply(log)\n",
    "    heuristic_net, heu_initial_marking, heu_final_marking = heuristics_miner.apply(log)\n",
    "    inductive_tree = inductive_miner.apply(log)\n",
    "    inductive_net, ind_initial_marking, ind_final_marking = pt_converter.apply(inductive_tree)\n",
    "\n",
    "    models = {\n",
    "        'Alpha': (alpha_net, alpha_initial_marking, alpha_final_marking),\n",
    "        'Heuristic': (heuristic_net, heu_initial_marking, heu_final_marking),\n",
    "        'Inductive': (inductive_net, ind_initial_marking, ind_final_marking)\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(log, models):\n",
    "    metrics = {}\n",
    "    \n",
    "    for name, (net, initial_marking, final_marking) in models.items():\n",
    "        fitness = replay_fitness_evaluator.apply(log, net, initial_marking, final_marking)\n",
    "        precision = precision_evaluator.apply(log, net, initial_marking, final_marking)\n",
    "        generalization = generalization_evaluator.apply(log, net, initial_marking, final_marking)\n",
    "        simplicity = simplicity_evaluator.apply(net)\n",
    "        \n",
    "        metrics[name] = {\n",
    "            'Fitness': fitness['average_trace_fitness'],\n",
    "            'Precision': precision,\n",
    "            'Generalization': generalization,\n",
    "            'Simplicity': simplicity\n",
    "        }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics_comparison(metrics):\n",
    "#     metrics_df = pd.DataFrame(metrics).T\n",
    "    \n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     metrics_df.plot(kind='bar', width=0.8)\n",
    "#     plt.title('Process Mining Metrics Comparison')\n",
    "#     plt.xlabel('Miners')\n",
    "#     plt.ylabel('Score')\n",
    "#     plt.legend(title='Metrics')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('metrics_comparison.png')\n",
    "#     plt.close()\n",
    "\n",
    "def plot_metrics_comparison(metrics, figures_folder):\n",
    "    # Convert metrics to DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics).T\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot bars\n",
    "    metrics_df.plot(kind='bar', width=0.8, ax=ax)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title('Process Mining Metrics Comparison')\n",
    "    plt.xlabel('Miners')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.subplots_adjust(right=0.85, bottom=0.15)\n",
    "    \n",
    "    # Save plot\n",
    "    output_path = os.path.join(figures_folder, f\"metrics_comparison.png\")\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Print numerical values in console\n",
    "    print(\"\\nNumerical Metrics:\")\n",
    "    print(metrics_df.round(3).to_string())\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_petri_nets(models, figures_folder):\n",
    "    os.makedirs(figures_folder, exist_ok=True)\n",
    "    for name, (net, initial_marking, final_marking) in models.items():\n",
    "        gviz = pn_visualizer.apply(net, initial_marking, final_marking)\n",
    "        output_path = os.path.join(figures_folder, f\"petri_net_{name.lower()}.png\")\n",
    "        pn_visualizer.save(gviz, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_user_habits(log):\n",
    "#     # Extract activity patterns\n",
    "#     activities_by_day = {}\n",
    "#     for trace in log:\n",
    "#         day = trace.attributes['concept:name']\n",
    "#         activities = [event['concept:name'] for event in trace]\n",
    "#         activities_by_day[day] = activities\n",
    "    \n",
    "#     # Analyze common patterns\n",
    "#     common_sequences = {}\n",
    "#     for day, activities in activities_by_day.items():\n",
    "#         sequence = ' -> '.join(activities)\n",
    "#         common_sequences[sequence] = common_sequences.get(sequence, 0) + 1\n",
    "    \n",
    "#     return common_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_user_habits(log):\n",
    "#     # Extract activity patterns by session\n",
    "#     activities_by_session = {}\n",
    "#     for trace in log:\n",
    "#         session_id = trace.attributes['concept:name']\n",
    "#         # Create list of (activity, state) pairs in temporal order\n",
    "#         activities = [(event['concept:name'], event['lifecycle:transition']) \n",
    "#                      for event in sorted(trace, key=lambda x: x['time:timestamp'])]\n",
    "#         activities_by_session[session_id] = activities\n",
    "    \n",
    "#     # Analyze common patterns\n",
    "#     common_sequences = {}\n",
    "#     for session_id, activities in activities_by_session.items():\n",
    "#         # Create meaningful sequence string with state transitions\n",
    "#         sequence_parts = []\n",
    "#         current_activity = None\n",
    "        \n",
    "#         for activity, state in activities:\n",
    "#             # Only add to sequence when activity changes or when same activity has different state\n",
    "#             if current_activity != activity:\n",
    "#                 sequence_parts.append(f\"{activity}({state})\")\n",
    "#                 current_activity = activity\n",
    "#             else:\n",
    "#                 sequence_parts.append(f\"â†’{state}\")\n",
    "        \n",
    "#         sequence = ' -> '.join(sequence_parts)\n",
    "#         common_sequences[sequence] = common_sequences.get(sequence, 0) + 1\n",
    "    \n",
    "#     # Sort by frequency\n",
    "#     sorted_sequences = dict(sorted(common_sequences.items(), \n",
    "#                                  key=lambda x: x[1], \n",
    "#                                  reverse=True))\n",
    "    \n",
    "#         # Save habits analysis\n",
    "#     with open('user_habits_analysis.txt', 'w') as f:\n",
    "#         for sequence, count in sorted(sorted_sequences.items(), key=lambda x: x[1], reverse=True):\n",
    "#             f.write(f'Frequency: {count}\\nSequence: {sequence}\\n\\n')\n",
    "    \n",
    "#     return sorted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_user_habits(log, processed_folder):\n",
    "    # Extract activity patterns by session\n",
    "    activities_by_session = {}\n",
    "    for trace in log:\n",
    "        session_id = trace.attributes['concept:name']\n",
    "        # Create list of (activity, state) pairs in temporal order\n",
    "        activities = [(event['concept:name'], event['lifecycle:transition']) \n",
    "                     for event in sorted(trace, key=lambda x: x['time:timestamp'])]\n",
    "        activities_by_session[session_id] = activities\n",
    "    \n",
    "    # Analyze common patterns\n",
    "    common_sequences = {}\n",
    "    for session_id, activities in activities_by_session.items():\n",
    "        # Create meaningful sequence string with state transitions\n",
    "        sequence_parts = []\n",
    "        current_activity = None\n",
    "        \n",
    "        for activity, state in activities:\n",
    "            # Only add new activities when they start (ON state)\n",
    "            if current_activity != activity and state == 'ON':\n",
    "                sequence_parts.append(activity)\n",
    "                current_activity = activity\n",
    "        \n",
    "        # Only create sequence if there are activities\n",
    "        if sequence_parts:\n",
    "            sequence = ' -> '.join(sequence_parts)\n",
    "            common_sequences[sequence] = common_sequences.get(sequence, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_sequences = dict(sorted(common_sequences.items(), \n",
    "                                 key=lambda x: x[1], \n",
    "                                 reverse=True))\n",
    "    \n",
    "    output_file = os.path.join(processed_folder, \"user_habits_analysis.txt\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sequence, count in sorted(sorted_sequences.items(), key=lambda x: x[1], reverse=True):\n",
    "            f.write(f'Frequency: {count}\\nSequence: {sequence}\\n\\n')\n",
    "    \n",
    "    return sorted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thuls\\AppData\\Local\\Temp\\ipykernel_10260\\1904810300.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_stratified = df.groupby('concept:name').apply(lambda x: x.sample(2)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "input_file = os.path.join('data', 'raw', 'tm001.txt')\n",
    "\n",
    "df = prepare_dataset(input_file)\n",
    "df_stratified = df.groupby('concept:name').apply(lambda x: x.sample(2)).reset_index(drop=True)\n",
    "\n",
    "# Convert to event log\n",
    "log = pm4py.convert_to_event_log(df_stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply process mining\n",
    "models = apply_process_mining(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8809635fe32f4c379e36a0447d78c5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6a81b86b964e1a883d40212dd93a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563aaaf245c047f5a15995a29d761af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9b3d37bea1459d909604e1ecd55076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062af6dadbf94a528ebdd4439d6e4c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c246b65c894c2eb4d081802e7cab4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e32d5e077f47ec8ca8f4ebd6dd64a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aligning log, completed variants ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14690a22ca7543ceab1f04d4e146cb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "computing precision with alignments, completed variants ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ba6794f2b146c2a6ea0edb279eab57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "metrics = calculate_metrics(log, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical Metrics:\n",
      "           Fitness  Precision  Generalization  Simplicity\n",
      "Alpha        0.317      0.685           0.293       0.417\n",
      "Heuristic    0.758      0.989           0.092       0.628\n",
      "Inductive    1.000      0.773           0.283       0.600\n"
     ]
    }
   ],
   "source": [
    "figures_folder = os.path.join('resources', 'figures')\n",
    "\n",
    "# Plot comparisons\n",
    "plot_metrics_comparison(metrics, figures_folder)\n",
    "\n",
    "# Visualize Petri nets\n",
    "visualize_petri_nets(models, figures_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze user habits\n",
    "user_habits_path = os.path.join('data', 'processed')\n",
    "habits = analyze_user_habits(log, user_habits_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
